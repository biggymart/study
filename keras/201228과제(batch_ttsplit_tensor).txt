2020년 12월 28일 과제

1. batch_size와 default 값에 대해서 알아보기
요약: 
batch_size란... 학습 한번 (1 epoch) 할 때 쓰이는 데이터의 크기 (한 꾸러미)
default value = 32

<학습 과정>
batch라는 영단어의 사전적 정의는 '일괄적'이란 뉘앙스를 많이 풍긴다: (1) (일괄적으로 처리되는) 집단, 무리 (2) 한 회분 (e.g. 한 번에 만들어 내는 음식기계 등의 양) (3) (일괄 처리를 위해) 함께 묶다.
batch_size는 "전체 트레이닝 데이터 셋을 여러 작은 그룹으로 나누었을 때, batch size는 하나의 소그룹에 속하는 데이터 수"를 의미함. 즉 한 번에 학습으로 입력시킬 "뭉텅이"의 크기임. 혹은 좀 공대스럽게 표현하면 "한 Step이 일어날 때 parallel하게 처리되는 data의 수"임. "학습 시의 일정 단위"

developers.google.com/machine-learning/glossary
배치(batch)
모델 학습의 반복 1회, 즉 경사 업데이트 1회에 사용되는 예의 집합입니다.

배치 크기 (batch size)
배치 하나에 포함되는 예의 개수입니다. 동적배치크기를 허용한다.

영문으로 찾아보니까, 한 article에서 이렇게 정의내림: (machinelearningmaster.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size)
"The number of examples from the training dataset used in the estimate of the error gradient"
하지만 error gradient (오류 경사하강법)이 뭔지 잘 모르겠다.

<밑바닥부터 시작하는 데이터 과학> p. 96보면 경사하강법의 개념이 나와있다. 함수에서 최솟값 혹은 최댓값을 찾는 방식 중 하나는, 임의의 시작점을 잡은 후 gradient(기울기, 혹은 "편미분의 벡터(??... 가장 큰 항의 계수를 말하는 건가? 문송하네)"를 계산하고 최솟값 혹은 최댓값을 찾기 위해 향하는 방식이라고 한다. 이 정도로만 이해하고 넘어가자. 
고등학교 때 극한을 떠올려보면, 기울기가 0보다 작은데 0으로 수렴하게 하면 local minimum에 갈거고, 반대로 기울기가 0보다 큰데 0으로 수렴하게 하면 local maximum으로 가겠지? 

영문 글로 돌아가보자면,
A training algorithm where the batch size is set to the total number of training examples is called  "batch gradient descent" 
>>> model.fit(train_x, train_y, batch_size=len(train_x))

and a training example algorithm where the batch size is set to 1 training example is called "stochastic/online gradient descent."
>>> model.fit(train_x, train_y, batch_size=1)

1~전체 데이터 셋 사이의 batch size는 "minibatch gradient descent"라고 불린다
>>> model.fit(train_x, train_y)
>>> model.fit(train_x, train_y, batch_size=32)

 그렇게 불린다고 한다. 암튼, batch size가 크면 한 epoch마다 들어가는 데이터의 뭉텅이가 그만큼 크다는 거고, 예측할 때 훈련 예제(training example)가 크면 클수록 더 예측이 정확하다고 한다. 

(...? 실습에서는 batch_size 너무 큰 값으로 넣으면 loss값이 더 커졌는데? 혼란스럽다) 아마 이 말이 혼란스러움을 좀 해결해주는 듯 하다 "Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error." 아, 너무 큰 batch size 주면 모델이 일반화의 오류를 범하는구나. 일반적으로 1에서 몇 백 사이의 값으로 batch size를 설정한다고 하고, 32 정도가 맨 처음에 주는 값으로 적당하다고 한다. "best training stability and generalization performance"

batch 한 번 들어가고 나오면 weight가 update된다고 하니까 지금까지 이해한 게 맞다면, 기계를 한 학생으로 비유해보자면, "학생이 한 번 문제풀이할 때마다 주어지는 문제의 개수" 정도로 이해하면 되겠다. 

수업에서는 누가 batch_size의 default value가 None이라고 했는데, 구글링 "devault batch size in keras"치니까 32라고 나오네. 32구만.

keras.rstudio.com/reference/fit.html
fit의 아규먼트 살펴보면
batch_size : interger or NULL. Number of samples per gradient update. If unspecified, (밑줄 쫙 쳐라) batch_size will default to 32.
땅땅땅. 됐다. 메뉴얼이 32이라고 하니까 맞네.
</학습과정>


2. train_test_split 완벽 정리
--> keras08_split5_size.py 참고

---이하 복사본---
from sklearn.model_selection import train_test_split
# 경우 1: train_size + test_size 값이 1보다 큰 경우
# x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.9, test_size=0.2, shuffle=False)
# ValueError: The sum of test_size and train_size = 1.1, should be in the (0, 1) range. Reduce test_size and/or train_size.
'''
경우 2: train_size + test_size 값이 1보다 작은 경우
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.7, test_size=0.2, shuffle=False)
x_train shape : (56,)
y_train shape : (56,)
x_val shape : (14,)
y_val shape : (14,)
x_test shape : (20,)
y_test shape : (20,)
에러는 뜨지 않지만 훈련 데이터와 검정 데이터의 숫자가 줄어들어 모델의 성능이 저하됨
(아래 train_size=0.8과 비교해볼 것)
loss : 8.109458923339844 mae : 2.8340137004852295
RMSE : 2.8477116465484613
R2 : 0.7561064173868346
'''
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=False)
# loss : 0.1361473649740219 mae : 0.364837646484375
# RMSE : 0.3689808094319125
# R2 : 0.9959053582637886

x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2)
print('x_train shape :', x_train.shape) # (64,)
print('y_train shape :', y_train.shape) # (64,)
print('x_val shape :', x_val.shape) # (16,)
print('y_val shape :', y_val.shape) # (16,)
print('x_test shape :', x_test.shape) # (20,)
print('y_test shape :', y_test.shape) # (20,)
---이상 복사본---

3.  스칼라 - 벡터 - 행렬 - 텐서 정리
순서대로, 1 - 2 - 3 - 4 차원

https://art28.github.io/blog/linear-algebra-1/ (아주 간단하게 설명했지만 부정확)
https://doooob.tistory.com/196 (좀 더 자세함)
스칼라: 하나의 숫자

벡터: 스칼라의 배열 (e.g. 수학에서 보는 2 by 1 행렬 꼴), 스칼라를 한 방향을 정렬한 것
수직 방향으로 늘어놓으면 "열 벡터"
가로 방향으로 늘어놓으면 "행 벡터" <- default name ("(아무 수식 없이) 벡터"라고 하면 이거)
벡터에 포함된 스칼라의 갯수가 벡터의 차원임. (e.g. 4차원 열벡터)
>>> x = np.array([1, 2, 3, 4])

행렬: 2차원의 배열 (e.g. 수학에서 보는 n by n 행렬 꼴)
같은 크기의 벡터를 복수로 늘어놓은 것
>>> A = np.array([[1, 2], [3, 4], [5, 6]]) # 3 by 2 행렬 꼴
>>> A.shape # 형태 확인할 수 있음, (3,2)

텐서: 2차원 이상의 배열, "임의의 차원을 갖는 배열"
벡터나 행렬을 일반화한 개념, "벡터는 1층 텐서이고 행렬은 2층 텐서"
e.g. RGB --> 3 layer tensor needed
https://programmers.co.kr/learn/courses/21/lessons/11051
